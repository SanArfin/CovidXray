{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3268049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# For classification performance metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c730c2d8",
   "metadata": {},
   "source": [
    "## images resized to 128 * 128, normalized, without masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81d820e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "base_path = r\"..\\\\data\\\\\"\n",
    "#base_path_in = os.path.join(base_path, \"processed\", \"extracted\")   # path to read from the *.npy files\n",
    "base_path_in = os.path.join(base_path, \"processed\")   # path to read from the *.npy files\n",
    "\n",
    "train_npy_file = 'train_data_resized_without_masks.npz'\n",
    "test_npy_file  = 'test_data_resized_without_masks.npz'\n",
    "\n",
    "#train data\n",
    "train = np.load(os.path.join(base_path_in, train_npy_file), allow_pickle=True)\n",
    "X_train = train['X_train']\n",
    "y_train = train['y_train']\n",
    "\n",
    "# test data\n",
    "test = np.load(os.path.join(base_path_in, test_npy_file), allow_pickle=True)\n",
    "X_test = test['X_test']\n",
    "y_test = test['y_test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ff88c72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35059\n",
      "35059\n",
      "4233\n",
      "4233\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "print(len(X_train))\n",
    "\n",
    "print(len(y_test))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae26047",
   "metadata": {},
   "source": [
    "## images resized to 128 * 128, normalized, with masks (incl. Gaussian Blur, Clahe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942acf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # define paths\n",
    "base_path = r\"..\\\\data\\\\\"\n",
    "#base_path_in = os.path.join(base_path, \"processed\", \"extracted\")   # path to read from the *.npy files\n",
    "base_path_in = os.path.join(base_path, \"processed\")   # path to read from the *.npy files\n",
    "\n",
    "train_npy_file = 'train_data_resized_with_masks.npz'\n",
    "test_npy_file  = 'test_data_resized_with_masks.npz'\n",
    "\n",
    "#train data\n",
    "train = np.load(os.path.join(base_path_in, train_npy_file), allow_pickle=True)\n",
    "X_train = train['X_train']\n",
    "y_train = train['y_train']\n",
    "\n",
    "# test data\n",
    "test = np.load(os.path.join(base_path_in, test_npy_file), allow_pickle=True)\n",
    "X_test = test['X_test']\n",
    "y_test = test['y_test'] \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac006ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35059\n",
      "35059\n",
      "4233\n",
      "4233\n"
     ]
    }
   ],
   "source": [
    "\"\"\" print(len(y_train))\n",
    "print(len(X_train))\n",
    "\n",
    "print(len(y_test))\n",
    "print(len(X_test)) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb681af4",
   "metadata": {},
   "source": [
    "## feature extraction with HOG based on images original size 299 * 299, normalized, with masks (incl. Gaussian Blur, Clahe) and apllied masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7af17bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # define paths\n",
    "base_path = r\"..\\\\data\\\\\"\n",
    "base_path_in = os.path.join(base_path, \"processed\", \"extracted\")   # path to read from the *.npy files\n",
    "\n",
    "train_npy_file = 'hog_features.npy'\n",
    "test_npy_file  = 'hog_features_test.npy'\n",
    "\n",
    "train_labels_file = 'labels_test.npy'\n",
    "test_labels_file = 'labels_train.npy'\n",
    "\n",
    "#train data\n",
    "X_train = np.load(os.path.join(base_path_in, train_npy_file))\n",
    "y_train = np.load(os.path.join(base_path_in, train_labels_file))\n",
    "\n",
    "# test data\n",
    "X_test = np.load(os.path.join(base_path_in, test_npy_file))\n",
    "y_test = np.load(os.path.join(base_path_in, test_labels_file))\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a068be37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4233\n",
      "35059\n",
      "35059\n",
      "4233\n"
     ]
    }
   ],
   "source": [
    "\"\"\" print(len(y_train))\n",
    "print(len(X_train))\n",
    "\n",
    "print(len(y_test))\n",
    "print(len(X_test)) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b5fda3",
   "metadata": {},
   "source": [
    "## first try of modeling with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "59de6b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sacling the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit and transform train data \n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# fit test data\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6f17043",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: unknown. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m clf = SVC(gamma=\u001b[32m0.01\u001b[39m,  kernel=\u001b[33m'\u001b[39m\u001b[33mpoly\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mclf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yvonne\\Documents\\DataScientist_2025_local\\project_code\\.venv-xray\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yvonne\\Documents\\DataScientist_2025_local\\project_code\\.venv-xray\\Lib\\site-packages\\sklearn\\svm\\_base.py:207\u001b[39m, in \u001b[36mBaseLibSVM.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    197\u001b[39m     X, y = validate_data(\n\u001b[32m    198\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    199\u001b[39m         X,\n\u001b[32m   (...)\u001b[39m\u001b[32m    204\u001b[39m         accept_large_sparse=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    205\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m sample_weight = np.asarray(\n\u001b[32m    210\u001b[39m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype=np.float64\n\u001b[32m    211\u001b[39m )\n\u001b[32m    212\u001b[39m solver_type = LIBSVM_IMPL.index(\u001b[38;5;28mself\u001b[39m._impl)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yvonne\\Documents\\DataScientist_2025_local\\project_code\\.venv-xray\\Lib\\site-packages\\sklearn\\svm\\_base.py:747\u001b[39m, in \u001b[36mBaseSVC._validate_targets\u001b[39m\u001b[34m(self, y)\u001b[39m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_validate_targets\u001b[39m(\u001b[38;5;28mself\u001b[39m, y):\n\u001b[32m    746\u001b[39m     y_ = column_or_1d(y, warn=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m747\u001b[39m     \u001b[43mcheck_classification_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    748\u001b[39m     \u001b[38;5;28mcls\u001b[39m, y = np.unique(y_, return_inverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    749\u001b[39m     \u001b[38;5;28mself\u001b[39m.class_weight_ = compute_class_weight(\u001b[38;5;28mself\u001b[39m.class_weight, classes=\u001b[38;5;28mcls\u001b[39m, y=y_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yvonne\\Documents\\DataScientist_2025_local\\project_code\\.venv-xray\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:222\u001b[39m, in \u001b[36mcheck_classification_targets\u001b[39m\u001b[34m(y)\u001b[39m\n\u001b[32m    214\u001b[39m y_type = type_of_target(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m    216\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    217\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    220\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmultilabel-sequences\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    221\u001b[39m ]:\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    223\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown label type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Maybe you are trying to fit a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    224\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mclassifier, which expects discrete classes on a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    225\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mregression target with continuous values.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Unknown label type: unknown. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values."
     ]
    }
   ],
   "source": [
    "clf = SVC(gamma=0.01,  kernel='poly')\n",
    "clf.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76271496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict  from X_test_scaled\n",
    "y_pred = clf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8c9f90",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c47ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.crosstab(y_test, y_pred, rownames=['Real Class'], colnames=['Predicted Class'])\n",
    "\n",
    "cm = confusion_matrix(y_test_class, y_pred_class)\n",
    "\n",
    "display(cm)\n",
    "\n",
    "sns.heatmap(cm, cmap='Blues', cbar=False, annot=True\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y_test_class, y_pred_class))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
