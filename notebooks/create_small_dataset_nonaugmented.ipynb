{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc00a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_small_dataset_nonaugmented.ipynb\n",
    "\n",
    "# This notebook creates a dataset which contains the resized images 128 x 128 pixel and has no augmented data.\n",
    "\n",
    "**input:**    \n",
    "../data/processed/df_xray_processed_normed_enc_test.cvs   (is created by train_test_split.ipynb)  \n",
    "../data/processed/df_xray_processed_normed_enc_train.cvs (is created by train_test_split.ipynb)\n",
    "\n",
    "It takes the filtered and normalized images which have masks included from folder \\normalized_xrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c7c3a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb04eac",
   "metadata": {},
   "source": [
    "## Some definitions which have to be set by the user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9e5d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths \n",
    "base_path = r\"..\\\\data\\\\\"\n",
    "base_path_out = os.path.join(base_path, \"processed\")   # path to read input csv-file from\n",
    "\n",
    "#output_path = os.path.join(base_path_out, \"non_augmented_with_masks_resized_128_128\") # path to write the\n",
    "\n",
    "\n",
    "# define number of pixel to which the images should be resized: \n",
    "num_pixel = 128\n",
    "\n",
    "# define number of images per class which the new smaller dataset should contain: \n",
    "num_subset_train = 1000\n",
    "\n",
    "# Define classes which need augmentation: \n",
    "#classes_aug = [\"COVID\", \"Viral Pneumonia\", \"Lung_Opacity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47db1d4",
   "metadata": {},
   "source": [
    "## images with masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cca32e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv with data frame which contains infos to preprocessed  and normalized images and labels and encoded labels\n",
    "df_train = pd.read_csv(os.path.join(base_path_out,\"df_xray_processed_normed_enc_train.csv\"), sep=',', index_col=0)\n",
    "df_test = pd.read_csv(os.path.join(base_path_out,\"df_xray_processed_normed_enc_test.csv\"), sep=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed1c4565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get num_subset images for each class\n",
    "\n",
    "num_subset_test  = df_test['label'].value_counts()['Viral Pneumonia'] # for test set take as many images as the smallest class has\n",
    "\n",
    "#print(df_y_train.columns)\n",
    "\n",
    "df_train_0 = df_train[(df_train['label_enc']==0)].sample(n=num_subset_train, replace = False, random_state = 42, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94c194d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m df_train_1 = df_train[(df_train[\u001b[33m'\u001b[39m\u001b[33mlabel_enc\u001b[39m\u001b[33m'\u001b[39m]==\u001b[32m1\u001b[39m)].sample(n=num_subset_train, replace = \u001b[38;5;28;01mFalse\u001b[39;00m, random_state = \u001b[32m42\u001b[39m, axis = \u001b[32m0\u001b[39m)\n\u001b[32m      8\u001b[39m df_train_2 = df_train[(df_train[\u001b[33m'\u001b[39m\u001b[33mlabel_enc\u001b[39m\u001b[33m'\u001b[39m]==\u001b[32m2\u001b[39m)].sample(n=num_subset_train, replace = \u001b[38;5;28;01mFalse\u001b[39;00m, random_state = \u001b[32m42\u001b[39m, axis = \u001b[32m0\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m df_train_3 = \u001b[43mdf_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel_enc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m==\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_subset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m df_train_subset = pd.concat([df_train_0, df_train_1, df_train_2, df_train_3], axis=\u001b[32m0\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# test\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yvonne\\Documents\\DataScientist_2025_local\\project_code\\.venv-xray\\Lib\\site-packages\\pandas\\core\\generic.py:6118\u001b[39m, in \u001b[36mNDFrame.sample\u001b[39m\u001b[34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[39m\n\u001b[32m   6115\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   6116\u001b[39m     weights = sample.preprocess_weights(\u001b[38;5;28mself\u001b[39m, weights, axis)\n\u001b[32m-> \u001b[39m\u001b[32m6118\u001b[39m sampled_indices = \u001b[43msample\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6119\u001b[39m result = \u001b[38;5;28mself\u001b[39m.take(sampled_indices, axis=axis)\n\u001b[32m   6121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yvonne\\Documents\\DataScientist_2025_local\\project_code\\.venv-xray\\Lib\\site-packages\\pandas\\core\\sample.py:152\u001b[39m, in \u001b[36msample\u001b[39m\u001b[34m(obj_len, size, replace, weights, random_state)\u001b[39m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    150\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInvalid weights: weights sum to zero\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m.astype(\n\u001b[32m    153\u001b[39m     np.intp, copy=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    154\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mnumpy\\\\random\\\\mtrand.pyx:1001\u001b[39m, in \u001b[36mnumpy.random.mtrand.RandomState.choice\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "# get num_subset images for each class\n",
    "\n",
    "num_subset_test  = df_test['label'].value_counts()['Viral Pneumonia'] # for test set take as many images as the smallest class has\n",
    "\n",
    "# train\n",
    "df_train_0 = df_train[(df_train['label_enc']==0)].sample(n=num_subset_train, replace = False, random_state = 42, axis = 0)\n",
    "df_train_1 = df_train[(df_train['label_enc']==1)].sample(n=num_subset_train, replace = False, random_state = 42, axis = 0)\n",
    "df_train_2 = df_train[(df_train['label_enc']==2)].sample(n=num_subset_train, replace = False, random_state = 42, axis = 0)\n",
    "df_train_3 = df_train[(df_train['label_enc']==3)].sample(n=num_subset_train, replace = False, random_state = 42, axis = 0)\n",
    "\n",
    "df_train_subset = pd.concat([df_train_0, df_train_1, df_train_2, df_train_3], axis=0)\n",
    "\n",
    "# test\n",
    "df_test_0 = df_test[(df_test['label_enc']==0)].sample(n=num_subset_test, replace = False, random_state = 42, axis = 0)\n",
    "df_test_1 = df_test[(df_test['label_enc']==1)].sample(n=num_subset_test, replace = False, random_state = 42, axis = 0)\n",
    "df_test_2 = df_test[(df_test['label_enc']==2)].sample(n=num_subset_test, replace = False, random_state = 42, axis = 0)\n",
    "df_test_3 = df_test[(df_test['label_enc']==3)].sample(n=num_subset_test, replace = False, random_state = 42, axis = 0)\n",
    "\n",
    "df_test_subset = pd.concat([df_test_0, df_test_1, df_test_2, df_test_3], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c2c9e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (1076, 16384)\n",
      "y_test shape: (1076,)\n",
      "Resized and flattened test images have been saved!\n",
      "X_train shape: (4000, 16384)\n",
      "y_train shape: (4000,)\n",
      "Resized and flattened train images have been saved!\n"
     ]
    }
   ],
   "source": [
    "# Build image paths for train and test data\n",
    "df_train_subset['image_path'] = df_train_subset.apply(lambda row: os.path.normpath(os.path.join(os.getcwd(), row['path'], row['file'])), axis=1)\n",
    "df_test_subset['image_path'] = df_test_subset.apply(lambda row: os.path.normpath(os.path.join(os.getcwd(), row['path'], row['file'])), axis=1)\n",
    "\n",
    "\n",
    "# Test\n",
    "\n",
    "# Load and preprocess the test images (resize and flatten)\n",
    "test_image_data = []\n",
    "for index, row in df_test_subset.iterrows():\n",
    "    img = cv2.imread(row['image_path'], cv2.IMREAD_GRAYSCALE)\n",
    "    img_resized = cv2.resize(img, (num_pixel, num_pixel))  # Resize image\n",
    "    img_flattened = img_resized.reshape(-1)  # Flatten image to 1D vector\n",
    "    if img_flattened is not None:\n",
    "        test_image_data.append(img_flattened) \n",
    "\n",
    "# Convert to NumPy array\n",
    "X_test = np.array(test_image_data, dtype=np.uint8)\n",
    "y_test = df_test_subset['label_enc'].to_numpy()\n",
    "\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Save the resized and flattened test data\n",
    "np.savez_compressed(os.path.join(base_path_out, 'test_non_augmented_with_masks_resized_128_128.npz'), X_test=X_test, y_test=y_test)\n",
    "print(\"Resized and flattened test images have been saved!\")\n",
    "\n",
    "\n",
    "# Train\n",
    "\n",
    "# Load and preprocess the train images (resize and flatten)\n",
    "train_image_data = []\n",
    "for index, row in df_train_subset.iterrows():\n",
    "    img = cv2.imread(row['image_path'], cv2.IMREAD_GRAYSCALE)\n",
    "    img_resized = cv2.resize(img, (num_pixel, num_pixel))  # Resize image\n",
    "    img_flattened = img_resized.reshape(-1)  # Flatten image to 1D vector\n",
    "    if img_flattened is not None:\n",
    "        train_image_data.append(img_flattened) \n",
    "\n",
    "# Convert to NumPy array\n",
    "X_train = np.array(train_image_data, dtype=np.uint8)\n",
    "y_train = df_train_subset['label_enc'].to_numpy()\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "# Save the resized and flattened train data\n",
    "np.savez_compressed(os.path.join(base_path_out, 'train_non_augmented_with_masks_resized_128_128.npz'), X_train=X_train, y_train=y_train)\n",
    "print(\"Resized and flattened train images have been saved!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370802e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
