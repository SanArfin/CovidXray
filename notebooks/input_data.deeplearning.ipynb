{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad3d4373",
   "metadata": {},
   "source": [
    "#train _test split(85% -15% )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d383115d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: Normal\n",
      "  Copied 8663 images to ..\\data\\processed\\train_data\\Normal\n",
      "  Copied 1529 images to ..\\data\\processed\\test_data\\Normal\n",
      "Processing class: COVID\n",
      "  Copied 3073 images to ..\\data\\processed\\train_data\\COVID\n",
      "  Copied 543 images to ..\\data\\processed\\test_data\\COVID\n",
      "Processing class: Lung_Opacity\n",
      "  Copied 5110 images to ..\\data\\processed\\train_data\\Lung_Opacity\n",
      "  Copied 902 images to ..\\data\\processed\\test_data\\Lung_Opacity\n",
      "Processing class: Viral Pneumonia\n",
      "  Copied 1143 images to ..\\data\\processed\\train_data\\Viral Pneumonia\n",
      "  Copied 202 images to ..\\data\\processed\\test_data\\Viral Pneumonia\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "random.seed(42)  # Set seed here for reproducibility\n",
    "source_dir = Path(\"../data/raw/COVID-19_Radiography_Dataset\")\n",
    "dest_dir = Path(\"../data/processed\")\n",
    "train_dir = dest_dir / \"train_data\"\n",
    "test_dir = dest_dir / \"test_data\"\n",
    "\n",
    "classes = ['Normal', 'COVID', 'Lung_Opacity', 'Viral Pneumonia']\n",
    "split_ratio = 0.85\n",
    "\n",
    "for cls in classes:\n",
    "    print(f\"Processing class: {cls}\")\n",
    "    cls_path = source_dir / cls / \"images\"  # <-- reading images from the correct place \n",
    "\n",
    "    all_images = [img for img in os.listdir(cls_path) if img.lower().endswith(('.png', '.jpg', '.jpeg'))]   #Reads all image filenames (ignoring other file types like .xml, .txt, etc.).\n",
    "    #Randomizes order so split is not biased.\n",
    "    random.shuffle(all_images)\n",
    "\n",
    "    split_index = int(len(all_images) * split_ratio)      #First 85% into train_imgs,Remaining 15% into test_imgs\n",
    "    train_imgs = all_images[:split_index]\n",
    "    test_imgs = all_images[split_index:]\n",
    "\n",
    "    # Create destination directories\n",
    "    train_cls_path = train_dir / cls\n",
    "    test_cls_path = test_dir / cls\n",
    "    train_cls_path.mkdir(parents=True, exist_ok=True)\n",
    "    test_cls_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Copy files\n",
    "    for img in train_imgs:\n",
    "        shutil.copy(cls_path / img, train_cls_path / img)\n",
    "    for img in test_imgs:\n",
    "        shutil.copy(cls_path / img, test_cls_path / img)\n",
    "\n",
    "    print(f\"  Copied {len(train_imgs)} images to {train_cls_path}\")\n",
    "    print(f\"  Copied {len(test_imgs)} images to {test_cls_path}\")\n",
    "\n",
    "#RESULT  OF SPLIT:\n",
    "    #Processing class: Normal\n",
    "  #Copied 8663 images to ..\\data\\processed\\train_data\\Normal\n",
    "  #Copied 1529 images to ..\\data\\processed\\test_data\\Normal\n",
    "#Processing class: COVID\n",
    "  #Copied 3073 images to ..\\data\\processed\\train_data\\COVID\n",
    "  #Copied 543 images to ..\\data\\processed\\test_data\\COVID\n",
    "#Processing class: Lung_Opacity\n",
    "  #Copied 5110 images to ..\\data\\processed\\train_data\\Lung_Opacity\n",
    "  #Copied 902 images to ..\\data\\processed\\test_data\\Lung_Opacity\n",
    "#Processing class: Viral Pneumonia\n",
    "  #Copied 1143 images to ..\\data\\processed\\train_data\\Viral Pneumonia\n",
    "  #Copied 202 images to ..\\data\\processed\\test_data\\Viral Pneumonia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490c0367",
   "metadata": {},
   "source": [
    "#augment minority and validation split, save train and val. npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0fde88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images and labels...\n",
      "  Loading 8663 images from 'Normal'\n",
      "  Loading 3073 images from 'COVID'\n",
      "  Loading 5110 images from 'Lung_Opacity'\n",
      "  Loading 1143 images from 'Viral Pneumonia'\n",
      "Total images loaded: 17989\n",
      "X shape: (17989, 64, 64, 1), y shape: (17989, 4)\n",
      "Minority class samples: 9326\n",
      "Augmented minority samples generated: 9326\n",
      "Total samples after augmentation: 27315\n",
      "Training samples: 21852, Validation samples: 5463\n",
      "Saved train and validation datasets as .npy files.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Settings ---\n",
    "data_dir = Path(\"../data/processed/train_data\")\n",
    "classes = ['Normal', 'COVID', 'Lung_Opacity', 'Viral Pneumonia']\n",
    "img_size = (64, 64)\n",
    "\n",
    "# --- Load images and labels ---\n",
    "X, y = [], []\n",
    "\n",
    "print(\"Loading images and labels...\")\n",
    "for class_index, class_name in enumerate(classes):\n",
    "    class_path = data_dir / class_name\n",
    "    img_files = list(class_path.glob(\"*\"))\n",
    "    print(f\"  Loading {len(img_files)} images from '{class_name}'\")\n",
    "\n",
    "    for img_path in img_files:\n",
    "        img = load_img(img_path, target_size=img_size, color_mode='grayscale')     #Load each image, resize to (64,64), convert to grayscale.\n",
    "\n",
    "        img_array = img_to_array(img)                        #Convert image to numpy array and append to list X.\n",
    "        X.append(img_array)      \n",
    "                                                            #Create one-hot encoded label vector and append to y.\n",
    "        label = np.zeros(len(classes))\n",
    "        label[class_index] = 1\n",
    "        y.append(label)\n",
    "\n",
    "X = np.array(X, dtype=np.float32)\n",
    "y = np.array(y, dtype=np.float32)                               #Convert Lists to Numpy Arrays\n",
    "print(f\"Total images loaded: {len(X)}\")\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "# --- Normalize ---\n",
    "X /= 255.0\n",
    "\n",
    "# --- Identify minority class samples (all except 'Normal') ---\n",
    "normal_mask = np.argmax(y, axis=1) == 0  # Index 0 is Normal\n",
    "minority_mask = ~normal_mask\n",
    "\n",
    "#The Normal class is majority.\n",
    "#Create masks to separate minority class samples (all classes except Normal).\n",
    "#Extract minority samples from X and y for augmentation.\n",
    "\n",
    "X_minority = X[minority_mask]\n",
    "y_minority = y[minority_mask]\n",
    "print(f\"Minority class samples: {len(X_minority)}\")\n",
    "\n",
    "# --- Data Augmentation for minority classes ---\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "datagen.fit(X_minority)\n",
    "\n",
    "#Generate Augmented Minority Samples\n",
    "batch_size = len(X_minority)\n",
    "\n",
    "augmented_iter = datagen.flow(\n",
    "    X_minority,\n",
    "    y_minority,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "augmented_minority = next(augmented_iter)\n",
    "print(f\"Augmented minority samples generated: {len(augmented_minority[0])}\")\n",
    "\n",
    "# --- Combine original and augmented data ---\n",
    "X_balanced = np.concatenate([X, augmented_minority[0]], axis=0)\n",
    "y_balanced = np.concatenate([y, augmented_minority[1]], axis=0)\n",
    "print(f\"Total samples after augmentation: {len(X_balanced)}\")\n",
    "\n",
    "# --- Split into train and validation sets with stratification ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_balanced,\n",
    "    y_balanced,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_balanced.argmax(axis=1)\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n",
    "\n",
    "# --- Save arrays as .npy files ---\n",
    "np.save('X_traindl.npy', X_train)\n",
    "np.save('y_traindl.npy', y_train)\n",
    "np.save('X_valdl.npy', X_val)\n",
    "np.save('y_valdl.npy', y_val)\n",
    "\n",
    "print(\"Saved train and validation datasets as .npy files.\")\n",
    "#X_train = np.load('X_traindl.npy.npy')\n",
    "#y_train = np.load('y_traindl.npy')\n",
    "#X_val = np.load('X_valdl.npy')\n",
    "#y_val = np.load('y_valdl.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90f5d58",
   "metadata": {},
   "source": [
    "#test npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e92c305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test images and labels...\n",
      "  Loading 1529 images from test class 'Normal'\n",
      "  Loading 543 images from test class 'COVID'\n",
      "  Loading 902 images from test class 'Lung_Opacity'\n",
      "  Loading 202 images from test class 'Viral Pneumonia'\n",
      "Total test images loaded: 3176\n",
      "Shape of X_test: (3176, 64, 64, 1), Shape of y_test: (3176, 4)\n",
      "Saved test datasets as .npy files.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define path to test data\n",
    "test_data_dir = Path(\"../data/processed/test_data\")\n",
    "\n",
    "# Define classes \n",
    "classes = ['Normal', 'COVID', 'Lung_Opacity', 'Viral Pneumonia']\n",
    "\n",
    "# Image size expected by the model \n",
    "img_size = (64, 64)\n",
    "\n",
    "# Prepare lists for images and labels\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "print(\"Loading test images and labels...\")\n",
    "for class_index, class_name in enumerate(classes):\n",
    "    class_path = test_data_dir / class_name\n",
    "    img_files = list(class_path.glob(\"*\"))\n",
    "    print(f\"  Loading {len(img_files)} images from test class '{class_name}'\")\n",
    "\n",
    "    for img_path in img_files:\n",
    "        img = load_img(img_path, target_size=img_size, color_mode='grayscale')\n",
    "        img_array = img_to_array(img)\n",
    "        X_test.append(img_array)\n",
    "\n",
    "        label = np.zeros(len(classes))\n",
    "        label[class_index] = 1\n",
    "        y_test.append(label)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_test = np.array(X_test, dtype=np.float32) / 255.0  # Normalize\n",
    "y_test = np.array(y_test, dtype=np.float32)\n",
    "\n",
    "print(f\"Total test images loaded: {len(X_test)}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}, Shape of y_test: {y_test.shape}\")\n",
    "\n",
    "# Save test data as .npy files\n",
    "np.save('X_testdl.npy', X_test)\n",
    "np.save('y_testdl.npy', y_test)\n",
    "\n",
    "print(\"Saved test datasets as .npy files.\")\n",
    "#Load them as:\n",
    "#X_test = np.load('X_testdl.npy')\n",
    "#y_test = np.load('y_testdl.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da76f8fe",
   "metadata": {},
   "source": [
    "'#model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d5fe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LENET, CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6642879d",
   "metadata": {},
   "source": [
    "#train and evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c196ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    verbose=2  # Cleaner output per epoch\n",
    ")\n",
    "\n",
    "# Evaluate on validation data\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val, batch_size=32, verbose=2)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=32, verbose=2)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
